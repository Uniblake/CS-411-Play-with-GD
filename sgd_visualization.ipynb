{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing SGD, Momentum, Adagrad and RMSProp\n",
    "\n",
    "# 可视化随机梯度下降，动量机制，适配学习率和RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By LongGang Pang@UC Berkeley\n",
    "\n",
    "使用 Jupyter Notebook 里的 ipywidgets 插件，演示者可以非常方便的生成交互动画。\n",
    "\n",
    "这个 Notebook 演示最原始的随机梯度下降算法，动量机制，梯度衰减，RMSprop, Adam 等。\n",
    "\n",
    "演示例子来自于 UC Berkeley 2019春季[深度学习课程课件](https://github.com/d2l-ai/berkeley-stat-157/blob/master/slides/5_2/momentum.ipynb)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker, cm\n",
    "import seaborn as sns\n",
    "from ipywidgets import *\n",
    "import math\n",
    "\n",
    "sns.set_context('paper', font_scale=2)\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_2d(x1, x2):\n",
    "    '''original function to minimize'''\n",
    "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def f_grad(x1, x2):\n",
    "    '''the gradient dfdx1 and dfdx2'''\n",
    "    dfdx1 = 0.2 * x1\n",
    "    dfdx2 = 4 * x2\n",
    "    return dfdx1, dfdx2\n",
    "\n",
    "def train_2d(trainer, lr):\n",
    "    \"\"\"Train a 2d object function with a customized trainer\"\"\"\n",
    "    x1, x2 = -5, -2\n",
    "    s_x1, s_x2 = 0, 0\n",
    "    res = [(x1, x2)]\n",
    "    for i in range(30):\n",
    "        x1, x2, s_x1, s_x2, lr = trainer(x1, x2, s_x1, s_x2, lr)\n",
    "        res.append((x1, x2))\n",
    "    return res\n",
    "\n",
    "def plot_2d(res, figsize=(10, 6), title=None):\n",
    "    x1_, x2_ = zip(*res)\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.plot([0], [0], 'r*', ms=15)\n",
    "    plt.text(0.0, 0.25, 'minimum', color='w')\n",
    "    plt.plot(x1_[0], x2_[0], 'ro', ms=10)\n",
    "    plt.text(x1_[0]+0.1, x2_[0]+0.2, 'start', color='w')\n",
    "    plt.plot(x1_, x2_, '-o', color='#ff7f0e')\n",
    "    \n",
    "    plt.plot(x1_[-1], x2_[-1], 'wo')\n",
    "    plt.text(x1_[-1], x2_[-1]-0.25, 'end', color='w')\n",
    "    \n",
    "    x1 = np.linspace(-5.5, 3, 50)\n",
    "    x2 = np.linspace(min(-3.0, min(x2_) - 1), max(3.0, max(x2_) + 1), 100)\n",
    "    x1, x2 = np.meshgrid(x1, x2)\n",
    "    plt.contourf(x1, x2, f_2d(x1, x2), cmap=cm.gnuplot)\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bee4838ab701436f9ef4f0beab0b0dcc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sgd(x1, x2, s1, s2, lr):\n",
    "    dfdx1, dfdx2 = f_grad(x1, x2)\n",
    "    return (x1 - lr * dfdx1, x2 - lr * dfdx2, 0, 0, lr)\n",
    "\n",
    "@interact(lr=(0, 1, 0.001))\n",
    "def visualize_gradient_descent(lr=0.05):\n",
    "    res = train_2d(sgd, lr)\n",
    "    plot_2d(res, title='SGD')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0a24d15a7949718a160e439e804d9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lr=(0, 0.99, 0.001), beta=(0, 0.99, 0.001),\n",
    "         continuous_update=False)\n",
    "def visualize_sgd_momentum(lr=0.1, beta=0.1):\n",
    "    '''lr: learning rate\n",
    "    beta: parameter for momentum sgd'''\n",
    "    \n",
    "    def momentum(x1, x2, v1, v2, lr):\n",
    "        dfdx1, dfdx2 = f_grad(x1, x2)\n",
    "        v1 = beta * v1 + lr * dfdx1\n",
    "        v2 = beta * v2 + lr * dfdx2\n",
    "        x1 = x1 - v1\n",
    "        x2 = x2 - v2\n",
    "        return (x1, x2, v1, v2, lr)\n",
    "    \n",
    "    res = train_2d(momentum, lr)\n",
    "    plot_2d(res, title='momentum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3e2108386e476a89a812ae57ef014d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lr=(0, 0.99, 0.01), beta=(0, 0.99, 0.01),\n",
    "          continuous_update=False)\n",
    "def visualize_sgd_inertia(lr=0.1, beta=0.1):\n",
    "    '''lr: learning rate\n",
    "    beta: parameter for inertia sgd'''\n",
    "    \n",
    "    def inertia(x1, x2, v1, v2, lr):\n",
    "        dfdx1, dfdx2 = f_grad(x1, x2)\n",
    "        v1 = beta * v1 + (1 - beta) * dfdx1\n",
    "        v2 = beta * v2 + (1 - beta) * dfdx2\n",
    "        x1 = x1 - lr * v1\n",
    "        x2 = x2 - lr * v2\n",
    "        return (x1, x2, v1, v2, lr)\n",
    "    \n",
    "    res = train_2d(inertia, lr)\n",
    "    plot_2d(res, title='inertia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b0e936c2f9474abb333ca2c4f6e8a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lr=(0, 4, 0.01),\n",
    "          continuous_update=False)\n",
    "def visualize_adagrad(lr=0.1):\n",
    "    '''lr: learning rate\n",
    "    beta: parameter for inertia sgd'''\n",
    "    def adagrad_2d(x1, x2, s1, s2, lr):\n",
    "        # The first two terms are the independent variable gradients\n",
    "        g1, g2 = f_grad(x1, x2)\n",
    "        eps = 1e-6\n",
    "        s1 += g1 ** 2\n",
    "        s2 += g2 ** 2\n",
    "        x1 -= lr / math.sqrt(s1 + eps) * g1\n",
    "        x2 -= lr / math.sqrt(s2 + eps) * g2\n",
    "        return x1, x2, s1, s2, lr\n",
    "    \n",
    "    res = train_2d(adagrad_2d, lr)\n",
    "    plot_2d(res, title='adagrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099eaa67909e4f1c9f903fa1140f9b6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lr=(0, 1, 0.001), \n",
    "          gamma=(0, 0.99, 0.001),\n",
    "          continuous_update=False)\n",
    "def visualize_adagrad(lr=0.001, gamma=0.9):\n",
    "    '''lr: learning rate\n",
    "    beta: parameter for inertia sgd'''  \n",
    "    def rmsprop_2d(x1, x2, s1, s2, lr):\n",
    "        eps = 1e-6\n",
    "        g1, g2 = f_grad(x1, x2)\n",
    "        s1 = gamma * s1 + (1 - gamma) * g1 ** 2\n",
    "        s2 = gamma * s2 + (1 - gamma) * g2 ** 2\n",
    "        x1 -= lr / math.sqrt(s1 + eps) * g1\n",
    "        x2 -= lr / math.sqrt(s2 + eps) * g2\n",
    "        return x1, x2, s1, s2, lr\n",
    "\n",
    "    res = train_2d(rmsprop_2d, lr)\n",
    "    plot_2d(res, title='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8eccf2df2a7401096de8cfbb5a00826"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lr=(0, 1, 0.001), \n",
    "          mu=(0, 0.99, 0.001),\n",
    "          nu=(0, 0.99, 0.001),\n",
    "          continuous_update=False)\n",
    "def visualize_adagrad(lr=0.1, mu=0.9, nu=0.999):\n",
    "    '''lr: learning rate\n",
    "    mu: parameter for E(g)\n",
    "    nu: parameter for E(g^2)\n",
    "    '''  \n",
    "    def Deltax(m, n, g, t):\n",
    "        eps = 1.0E-6\n",
    "        m = mu * m + (1 - mu) * g\n",
    "        n = nu * n + (1 - nu) * g*g\n",
    "        m_hat = m / (1 - mu**t)\n",
    "        n_hat = n / (1 - nu**t)\n",
    "        dx = lr * m_hat / (math.sqrt(n_hat) + eps)\n",
    "        return m, n, dx\n",
    "        \n",
    "    def adam_2d(x1, x2, m1, n1, m2, n2, lr, t):\n",
    "        '''m1, m2: E(g1), E(g2)\n",
    "           n1, n2: E(g1^2), E(g2^2) where E() is expectation\n",
    "           lr: learning rate\n",
    "           t: time step'''\n",
    "        eps = 1e-6\n",
    "        g1, g2 = f_grad(x1, x2)\n",
    "        m1, n1, dx1 = Deltax(m1, n1, g1, t)\n",
    "        m2, n2, dx2 = Deltax(m2, n2, g2, t)       \n",
    "        x1 -= dx1\n",
    "        x2 -= dx2\n",
    "        return x1, x2, m1, n1, m2, n2, lr\n",
    "    \n",
    "    def train_adam(trainer, lr):\n",
    "        \"\"\"Train a 2d object function with a customized trainer\"\"\"\n",
    "        x1, x2 = -5, -2\n",
    "        m1, n1, m2, n2 = 0, 0, 0, 0\n",
    "        res = [(x1, x2)]\n",
    "        for i in range(30):\n",
    "            x1, x2, m1, n1, m2, n2, lr = trainer(x1, x2, m1, n1, m2, n2, lr, i+1)\n",
    "            res.append((x1, x2))\n",
    "        return res\n",
    "    \n",
    "    res = train_adam(adam_2d, lr)\n",
    "    plot_2d(res, title='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
